# Creative Muse AI - Multi-Model Support

Das Creative Muse System unterst√ºtzt jetzt mehrere AI-Modelle gleichzeitig und erm√∂glicht dynamisches Wechseln zwischen verschiedenen Modellen.

## üöÄ Features

### ‚úÖ Implementiert

- **Model Manager**: Zentrale Verwaltung aller AI-Modelle
- **Dynamisches Model-Switching**: Wechseln zwischen Modellen zur Laufzeit
- **Multi-Model Backend**: `main_multi_model.py` mit erweiterten APIs
- **Frontend Model-Selector**: UI-Komponente zur Modell-Auswahl
- **Model-spezifische Konfiguration**: Individuelle Parameter pro Modell
- **Speicher-Management**: Automatisches Laden/Entladen von Modellen
- **Model-Status-Tracking**: √úberwachung des Modell-Status
- **Download-Script**: Automatisierter Model-Download

### üîß Architektur

```
ai_core/
‚îú‚îÄ‚îÄ model_manager.py          # Zentrale Model-Verwaltung
‚îú‚îÄ‚îÄ main_multi_model.py       # Multi-Model Backend
‚îú‚îÄ‚îÄ main_llm.py              # Single-Model Backend (Legacy)
‚îî‚îÄ‚îÄ requirements.txt         # Erweiterte Dependencies

creative-muse-modern/
‚îú‚îÄ‚îÄ src/lib/api.ts           # Erweiterte API-Schnittstelle
‚îú‚îÄ‚îÄ src/components/
‚îÇ   ‚îî‚îÄ‚îÄ ModelSelector.tsx    # Model-Auswahl UI
‚îî‚îÄ‚îÄ src/app/
    ‚îú‚îÄ‚îÄ page.tsx            # Hauptseite mit Model-Support
    ‚îî‚îÄ‚îÄ stats/page.tsx      # Statistiken mit Model-Info

scripts/
‚îú‚îÄ‚îÄ download_models.py       # Model-Download Script
‚îî‚îÄ‚îÄ README_MODELS.md        # Model-Management Guide
```

## ü§ñ Unterst√ºtzte Modelle

### Empfohlene Modelle

| Modell | Gr√∂√üe | Token | Beschreibung |
|--------|-------|-------|-------------|
| **Mistral-7B-Instruct-v0.3** | 14.5GB | ‚úÖ | Hauptmodell f√ºr Creative Muse |
| Mistral-7B-Instruct-v0.2 | 14.5GB | ‚úÖ | Alternative Mistral Version |

### Test-Modelle

| Modell | Gr√∂√üe | Token | Beschreibung |
|--------|-------|-------|-------------|
| Microsoft DialoGPT-Medium | 1.5GB | ‚ùå | Freies Modell f√ºr Tests |
| Microsoft DialoGPT-Large | 3GB | ‚ùå | Gr√∂√üeres freies Modell |

## üîß Setup & Verwendung

### 1. Backend starten

```bash
# Multi-Model Backend
cd ai_core
python main_multi_model.py

# Oder Legacy Single-Model
python main_llm.py
```

### 2. Modelle herunterladen

```bash
# Empfohlenes Modell
python scripts/download_models.py --download mistral-7b-instruct-v0.3

# Alle empfohlenen Modelle
python scripts/download_models.py --download-all

# Verf√ºgbare Modelle anzeigen
python scripts/download_models.py --list
```

### 3. Frontend starten

```bash
cd creative-muse-modern
npm run dev
```

## üì° API-Endpunkte

### Model-Management

```http
GET /api/v1/models
# Hole alle verf√ºgbaren Modelle

POST /api/v1/models/switch
# Wechsle zu anderem Modell
{
  "model_key": "mistral-7b-instruct-v0.3"
}
```

### Idea-Generation

```http
POST /api/v1/generate
# Generiere Idee mit spezifischem Modell
{
  "prompt": "Innovative Startup-Idee",
  "category": "business",
  "creativity_level": 8,
  "language": "de",
  "model": "mistral-7b-instruct-v0.3",
  "max_tokens": 512,
  "temperature": 0.8
}

POST /api/v1/random
# Zuf√§llige Idee mit Modell-Auswahl
{
  "category": "technology",
  "language": "en",
  "model": "mistral-7b-instruct-v0.3"
}
```

### Statistiken

```http
GET /api/v1/stats
# Erweiterte Statistiken mit Model-Info
{
  "total_ideas": 42,
  "categories": {...},
  "model_stats": {
    "total_models": 4,
    "available_models": 2,
    "loaded_models": 1,
    "current_model": "mistral-7b-instruct-v0.3",
    "model_status": {...},
    "memory_usage": {...}
  }
}
```

## üéØ Frontend-Features

### Model-Selector Komponente

- **Modell-√úbersicht**: Alle verf√ºgbaren und nicht verf√ºgbare Modelle
- **Status-Anzeige**: Aktueller Status jedes Modells
- **Ein-Klick-Wechsel**: Einfaches Wechseln zwischen Modellen
- **Download-Hinweise**: Anweisungen f√ºr fehlende Modelle
- **Empfehlungen**: Hervorhebung empfohlener Modelle

### Erweiterte Ideen-Anzeige

- **Model-Badge**: Zeigt verwendetes Modell pro Idee
- **Generation-Method**: Unterscheidung zwischen Modell-Typen
- **Performance-Tracking**: Modell-spezifische Statistiken

## ‚öôÔ∏è Konfiguration

### Model-Konfiguration

```python
# model_manager.py
ModelConfig(
    key="mistral-7b-instruct-v0.3",
    name="mistralai/Mistral-7B-Instruct-v0.3",
    model_path="mistral-7b-instruct-v0.3",
    description="Mistral 7B Instruct v0.3 - Hauptmodell",
    size_gb=14.5,
    requires_token=True,
    recommended=True,
    max_tokens=512,
    temperature=0.7,
    top_p=0.9,
    device_preference="auto"
)
```

### Umgebungsvariablen

```bash
# .env
HF_TOKEN=your_huggingface_token
MODEL_CACHE_DIR=./models
API_HOST=localhost
API_PORT=8000
```

## üîÑ Model-Switching Workflow

1. **Frontend**: Benutzer w√§hlt Modell im ModelSelector
2. **API-Call**: `POST /api/v1/models/switch`
3. **Backend**: ModelManager l√§dt neues Modell
4. **Memory-Management**: Altes Modell optional entladen
5. **Status-Update**: Frontend erh√§lt neuen Status
6. **Idea-Generation**: Neue Ideen verwenden gew√§hltes Modell

## üìä Performance & Memory

### Speicher-Optimierung

- **Lazy Loading**: Modelle werden nur bei Bedarf geladen
- **Memory Cleanup**: Automatisches Entladen nicht verwendeter Modelle
- **Device Management**: Intelligente CPU/GPU-Zuweisung
- **Cache Control**: Optimierte Speicher-Nutzung

### Monitoring

- **Model Status**: Echtzeit-Status aller Modelle
- **Memory Usage**: CPU/GPU-Speicher-√úberwachung
- **Performance Metrics**: Generierungs-Geschwindigkeit
- **Error Tracking**: Modell-spezifische Fehlerbehandlung

## üö® Troubleshooting

### H√§ufige Probleme

**Modell l√§dt nicht**
```bash
# Pr√ºfe HF_TOKEN
echo $HF_TOKEN

# Teste Model-Download
python scripts/download_models.py --test mistral-7b-instruct-v0.3
```

**Out of Memory**
```python
# Reduziere max_memory in model_manager.py
max_memory={"cpu": "4GB"}  # Statt 6GB

# Oder verwende kleineres Modell
python scripts/download_models.py --download microsoft-dialoGPT-medium
```

**Model Switch fehlgeschlagen**
```bash
# Pr√ºfe Backend-Logs
tail -f logs/backend.log

# Restart Backend
pkill -f main_multi_model.py
python main_multi_model.py
```

## üîÆ Roadmap

### Geplante Features

- [ ] **Model-Ensembles**: Kombiniere mehrere Modelle
- [ ] **A/B Testing**: Vergleiche Modell-Performance
- [ ] **Custom Models**: Support f√ºr eigene Fine-tuned Modelle
- [ ] **Model Metrics**: Detaillierte Performance-Metriken
- [ ] **Auto-Scaling**: Automatisches Model-Management
- [ ] **Distributed Models**: Multi-Server Model-Hosting

### Verbesserungen

- [ ] **Streaming Generation**: Echtzeit-Text-Streaming
- [ ] **Batch Processing**: Mehrere Ideen gleichzeitig
- [ ] **Model Preloading**: Intelligentes Vorladen
- [ ] **Quality Scoring**: Automatische Ideen-Bewertung
- [ ] **Context Awareness**: Modell-spezifische Prompts

## üìö Weitere Dokumentation

- [`scripts/README_MODELS.md`](../scripts/README_MODELS.md) - Model Download Guide
- [`docs/DEPLOYMENT.md`](DEPLOYMENT.md) - Deployment-Anleitung
- [`docs/NEXTJS_FRONTEND.md`](NEXTJS_FRONTEND.md) - Frontend-Dokumentation
- [`README.md`](../README.md) - Haupt-Dokumentation

## ü§ù Beitragen

Neue Modelle hinzuf√ºgen:

1. Erweitere `AVAILABLE_MODELS` in `download_models.py`
2. F√ºge `ModelConfig` in `model_manager.py` hinzu
3. Teste Download und Integration
4. Aktualisiere Dokumentation
5. Erstelle Pull Request

Das Multi-Model-System macht Creative Muse flexibler und leistungsf√§higer! üöÄ